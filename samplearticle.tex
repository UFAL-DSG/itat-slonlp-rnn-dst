\documentclass{itatnew}
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}
\presetkeys{todonotes}{prepend}{}
\presetkeys{todonotes}{caption=TODO}{}

\def\OP#1{\textcolor{purple}{OP: \textit{#1}}}
% \def\todo#1{\textcolor{purple}{OP: \textit{#1}}}

\begin{document}

\title{Recurrent neural networks for dialog state tracking}

\author{Ondřej Plátek \and Petr Bělohlávek \and Vojtěch Hudeček \and
Josef Válek \and TODOFilip}

\institute{Charles University in Prague,\\
\email{oplatek@ufal.mff.cuni.cz},\\ 
\texttt{http://ufal.mff.cuni.cz/ondrej-platek}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
This paper discuss models for dialog state tracking using recurrent neural networks (RNN).
We present experiments on standard dialog state tracking (DST) dataset DSTC2\cite{todo}.
On one hand, RNN models became state of the art in DST,
on the other hand most state-of-the-art models are only turn-based and require preprocessing specific to evaluated dataset (e.g. DSTC2) to achieve state-of-the-art results.
We implemented three architectures which can be used in incremental settings and requires almost no preprocessing.
We compare their performance to the benchmarks on DSTC2 and discuss their properties.
With only a trivial preprocessing we were able to beat a baseline, but our models are not competitive with state-of-the-art.
\end{abstract}
%
\section{Introduction}
%
The dialog state tracking (DST) is a standard and important task for evaluating conversational agents\cite{dstc1, dstc2, dstc3, HIS model}.
The dialog state tracker summarizes hidden information state (HIS) \cite{todo} of users goal from the conversation history.
Users goals are expressed in a~formal language typically represented as a dialog act item (DAI) $(actionType, slotName, slotValue)$.
It was shown that the better dialog state tracking of HIS the conversation agents are able to achieve the better success rate they have in overall completion of the task oriented conversation.\cite{Steve Yound, Henderson,..Zilka, Alex}
The dialog state tracking translates ambiguous natural language into formal language which is convenient for reasoning and accessing external knowledges, both important aspects of successful conversation in task oriented dialog.

Most state-of-the-art system in DST have reported their performance on Dstc2 dataset\cite{dstc2henderson}. 
The full dataset is freely available since January 2014 and contains X dialogues in training set, Y dialogues in dev set and Z dialogues test set.
The conversation are annotated at turn level where the hidden information state was annotated manually in for of $(actionType, slotName, slotValue)$
according a domain ontology.
The ontology of Dstc2 captures a~restaurant domain and was also manually designed.
The dataset also contains a database of restaurants and their properties.

Our experiments use only history \todo{ASR or } or gold transcriptions of conversation history and information of database values to predict dialog state which simplify setup and still obtain reasonable results.
We argue that using only database values instead of full ontology is natural, because the system can inform users only about the values in the database.
In addition we show in Section \ref{todo} it does not harm the performance on the dataset.

In our experiments we focus only on the {\it goal} slots predictions because the other groups are trivial to predict\footnote{The slots {\it Requested} and {\it Method} have accuracies 0.95 and 0.95 on the set according to state-of-the-art\cite{JWilliams}.} and such obtaining annotated data for new domain is significantly simpler.
We show that limiting ourself to {\it goal} slots which track the properties of restaurant wanted by the user  and in particular to their values which are only in the database, we do not loose much accuracy.
See Section~\ref{todo} for details.
Tracking only constraints of the restaurants in the dialog state allow us to annotate the dialog state as query to database, which is much simpler interface for human annotators than selecting many options from Dstc2 ontology.
Simplifying the dialog state while maintaining the accuracy allows simpler portability of proposed models to new domains.

We also show that Dstc2 dataset suffers from difference between training, development and test data.
The Dstc2 test set was collected in \todo{different scenario} \cite{dstc2henderson}.
Since the data of training, development and test set are distributed differently the resulting performance between training and test accuracy is rather high. 
Our experiments showed that by random resplitting of the Dstc2 data one obtain much better results.
As a result, we think that Dstc2 might suggest too pessimistic view of state-of-the-art methods in dialog state tracking because of the data distribution mismatch.

Our contribution is two fold. 
First, we compare three different architectures using RNN for dialog state tracking in Section~\ref{todo}.
Second, we analyze the Dstc2 dataset and propose changes for further data collections of similar datasets.


\section{Models}

Our models are all based on Recurrent Neural Network encoder\cite{todo} and similarly to RNN encoder\cite{Lukas} the models update its hidden state $h_{enc}$ after each word.
On the other hand, the models differ how they predict {\it goal} labels $food$, $area$ and $price range$ from the encoded state.

The first model predicts the triples of slot values $(food, area, price range)$ jointly from the encoder hidden state $h_{enc}$.
The second model predicts the labels independently employing a three classifiers where each of the predicts either $food$, $area$ or $price range$ based on $h_{enc}$. 
The last model uses a decoder for predicting values one after each other from the $h_{enc}$ and the whole model is implementation of encoder-decoder model successfully used in machine translation\cite{bahdanou}.

We implemented all three models in TensorFlow\cite{tensorflow} framework and by introducing more and more complex model we tried to balance data sparsity problem and incorrect independence assumptions.

\subsection{Prediction labels jointly}
todo describe encoder -how featers are fead
decoding - directly predicting triples
problem unseen combination on dev data/test data

\subsection{Predicting labels independently}
Encoding input features into encoder hidden state  

\subsection{Encoder decoder framework}
Encoding input features into encoder hidden state is the same.
We use attention

\section{Experiments}

\subsection{training}
feeding data - history prefix, turn by turn

\subsection{Labels representation evaluation}

\begin{table}
\caption{Accuracy on development and {\it Dstc2 test set }}
\begin{center}
\begin{tabular}{r@{\quad}rl}
\hline
\multicolumn{1}{l}{\rule{0pt}{12pt}
                   Year}&\multicolumn{2}{l}{World population}\\[2pt]
\hline\rule{0pt}{12pt}
8000 B.C.  &     5,000,000& \\
  50 A.D.  &   200,000,000& \\
1650 A.D.  &   500,000,000& \\
1945 A.D.  & 2,300,000,000& \\
1980 A.D.  & 4,400,000,000& \\[2pt]
\hline
\end{tabular}
\end{center}
\end{table}


\begin{figure}
\includegraphics[width=0.5\textwidth]{encoder}
\caption{The RNN encodes the word history into dialog state $h_T$ and predicts slot values independently.}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{encoder_joint}
\caption{The joint label predictions using RNN.}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{encdec}
\caption{Encoder decoder with attention predicts goals.}
\end{figure}

\subsection{Data preparation experiments}

\section{Related work}
Our system is related to RNN tracker of \cite{Zilka} which reported near state-of-the art result on Dstc2 dataset and was the first incremental system which was able to update the dialog state word-by-word with such accuracy.
In contrast to work of \cite{Zilka} we use no abstraction of slot values but we add the additional features as described in Section~\ref{todo_model_input_featurs}.
The first system which used Neural Network for dialog state tracking \cite{henderson} used feed forward network and manually engineered more than ten features across different levels of abstraction of the user input including spoken language understanding component (SLU).
In our work we focus on simplifying the architecture and so we use only features which are explicitly given by the dialog history word representation and the task database.

The system of \cite{Word-based dialog state tracking with recurrent neural networks-Henderson} gives state-of-the-art results and like our system it predicts the dialog state from words using a recurrent neural networks.
On the other hand their system heavily relies on abstracting user input.
Another dialog state tracker with LSTM was used in reinforcement setting but they also used information from SLU pipeline.\cite{Dialog History Construction with Long-Short Term Memory for Robust Generative Dialog State Tracking}

It is worth noting that there are first attempts to train end-to-end dialog system even without explicitly modelling dialog state\cite{Weston} which simplifies the overall architecture of a dialog system.
However, the end-to-end model was evaluated only on artificial dataset and cannot be compared to dstc2 dataset directly.

\section{Conclusion}




\paragraph{Acknowledgement}
We would like to thank nvidia, gauk, filipgrant.




%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%
\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. {\bf 78} (1982) 315--333

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris {\bf 287} (1978) 1013--1015

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. {\bf 72} (1988) 28--55

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. {\bf 33} (1980) 609--633

% JasonDWilliams,“Web-stylerankingandslucombina- tion for dialog state tracking,” in 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 282.

\end{thebibliography}
\end{document}
